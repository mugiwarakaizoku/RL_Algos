# -*- coding: utf-8 -*-
"""DDPG_CartPole.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14pW_bp7s3BQSAjyMfG2L95hDfs8DGbBX
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

import gym
import matplotlib.pyplot as plt 
import numpy as np
from collections import deque
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
import random
import keras

tf.version

env = gym.make('Pendulum-v0')

BATCH_SIZE=32
GAMMA=0.98
LEARNING_RATE_ACTOR=0.001
LEARNING_RATE_CRITIC = 0.002
UPDATE_TARGET_WEIGHTS_AFTER=30
REPLAY_MEMORY_SIZE=50000
TOTAL_EPISODES =5000
RHO = 0.005
NOISE = 0.1

total_actions = 3
input_size = len(env.observation_space.sample())

class Replay_Memory():
    def __init__(self):
        self.buffer = deque(maxlen=REPLAY_MEMORY_SIZE)
        
    def add_transition(self,transition):
        self.buffer.append(transition)
        
    def sample(self):
        batch_size = min(BATCH_SIZE, len(self.buffer))
        batch = random.sample(self.buffer,batch_size)
        state_list,action_list,reward_list,next_state_list,done_list = [],[],[],[],[]
        for transition in batch:
            state_list.append(transition[0])
            action_list.append(transition[1])
            reward_list.append(transition[2])
            next_state_list.append(transition[3])
            done_list.append(transition[4])
        return np.array(state_list),action_list,reward_list,np.array(next_state_list),done_list
    
    def size(self):
        return len(self.buffer)

class Critic_Network(keras.Model):
    def __init__(self,fc1_dims=64,fc2_dims=64):
        super(Critic_Network,self).__init__()
        self.fc1_dims = fc1_dims
        self.fc2_dims = fc2_dims
        self.fc1 = Dense(self.fc1_dims,activation='relu')
        self.fc2 = Dense(self.fc2_dims,activation='relu')
        self.q = Dense(1,activation=None)
    def call(self,state,action):
        q_val = self.fc1(tf.concat([state,action],axis=1))
        q_val = self.fc2(q_val)
        q_val = self.q(q_val)
        return q_val

class Actor_Network(keras.Model):
    def __init__(self,n_actions=total_actions,fc1_dims=64,fc2_dims=64):
        super(Actor_Network,self).__init__()
        self.fc1_dims = fc1_dims
        self.fc2_dims = fc2_dims
        self.n_actions = total_actions
        self.fc1 = Dense(self.fc1_dims,activation='relu')
        self.fc2 = Dense(self.fc2_dims,activation='relu')
        self.pi = Dense(self.n_actions,activation='tanh')
    def call(self,state):
        pi_val = self.fc1(state)
        pi_val = self.fc2(pi_val)
        pi_val = self.pi(pi_val)
        return pi_val

class DDPG_Agent():
    def __init__(self):
        self.memory = Replay_Memory()
        self.max_action=env.action_space.high[0]
        self.min_action = env.action_space.low[0]
        self.actor = Actor_Network()
        self.critic = Critic_Network()
        self.target_actor = Actor_Network()
        self.target_critic = Critic_Network()
        self.actor.compile(optimizer = Adam(learning_rate=LEARNING_RATE_ACTOR))
        self.target_actor.compile(optimizer = Adam(learning_rate=LEARNING_RATE_ACTOR))
        self.critic.compile(optimizer = Adam(learning_rate=LEARNING_RATE_CRITIC))
        self.target_critic.compile(optimizer = Adam(learning_rate=LEARNING_RATE_CRITIC))
    
    def chose_action(self,state):
        state_tensor = tf.convert_to_tensor(state[None,:])
        action = self.actor(state_tensor)
        action+=tf.random.normal(shape=[total_actions],mean=0,stddev =NOISE)
        return action[0]
        
    def train(self,mini_batch):
        state_batch, action_batch, reward_batch, next_state_batch,done_batch = mini_batch
        with tf.GradientTape() as tape:
            pi_target = self.target_actor.call(next_state_batch)
            q_target = tf.squeeze(self.target_critic.call(next_state_batch,pi_target),1)
            q = tf.squeeze(self.critic.call(state_batch,action_batch),1)
            target = [r+GAMMA*q_tar*(1-terminal_) for r,q_tar,terminal_ in zip(reward_batch,q_target,done_batch)]
            critic_loss = (target-q)**2
        critic_network_gradient = tape.gradient(critic_loss,self.critic.trainable_variables)
        self.critic.optimizer.apply_gradients(zip(critic_network_gradient,self.critic.trainable_variables))
        with tf.GradientTape() as tape:
            pi = self.actor.call(state_batch)
            actor_loss = -self.critic.call(state_batch,pi)
            actor_loss = tf.math.reduce_mean(actor_loss)
        actor_network_gradient = tape.gradient(actor_loss,self.actor.trainable_variables)
        self.actor.optimizer.apply_gradients(zip(actor_network_gradient,self.actor.trainable_variables))
        self.update_network_weights()
            
            
    def update_network_weights(self):
        weights = []
        targets = self.target_actor.weights
        for i,weight in enumerate(self.actor.weights):
            weights.append(weight*RHO+targets[i]*(1-RHO))
        self.target_actor.set_weights(weights)
        weights = []
        targets = self.target_critic.weights
        for i,weight in enumerate(self.critic.weights):
            weights.append(weight*RHO+targets[i]*(1-RHO))
        self.target_critic.set_weights(weights)

"""### Model Training"""

agent = DDPG_Agent()
memory = Replay_Memory()
score_list = []
score = 0
for n_ep in range(TOTAL_EPISODES):
    curr_state = env.reset()
    done=False
    while not done:
        action = agent.chose_action(curr_state)
        next_state,reward,done,_=env.step(action)
        score+=reward
        memory.add_transition((curr_state,action,reward,next_state,done))
        curr_state = next_state
    score_list.append(score)
    sample_batch = memory.sample()
    loss = agent.train(sample_batch)
     agent.update_network_weights()
     print('num_eps: {} loss: {} score: {}'.format(n_ep,loss,score/UPDATE_TARGET_WEIGHTS_AFTER))
     score=0
env.close()

plt.plot(score_list)
plt.ylabel('Avg_Score')
plt.xlabel('Episode')
plt.show()

