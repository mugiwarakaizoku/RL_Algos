# -*- coding: utf-8 -*-
"""PPO.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y0ih9i6y64hkBM-pL-QaOoWqt16wQPAU
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

import gym
import matplotlib.pyplot as plt 
import numpy as np
from collections import deque
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
import random
import keras
import tensorflow_probability as tfp

tf.version

env = gym.make('CartPole-v1')

BATCH_SIZE=32
GAMMA=0.98
LEARNING_RATE_ACTOR=0.001
LEARNING_RATE_CRITIC = 0.002
REPLAY_MEMORY_SIZE=300
TOTAL_EPISODES =1000
POLICY_CLIP = 0.2
LAMBDA = 0.95
EPOCHS = 4

total_actions = env.action_space.n
input_size = len(env.observation_space.sample())

class Replay_Memory():
  def __init__(self):
    self.buffer = deque(maxlen=REPLAY_MEMORY_SIZE)

  def add_transition(self,transition):
    self.buffer.append(transition)

  def sample(self):
    batch_size = min(BATCH_SIZE,len(self.buffer))
    batch = random.sample(self.buffer,batch_size)
    state_list,action_list,reward_list,next_state_list,done_list,prob_list_s,val_list_s = [],[],[],[],[],[],[]
    for transition in batch:
      state_list.append(transition[0])
      action_list.append(transition[1])
      reward_list.append(transition[2])
      next_state_list.append(transition[3])
      done_list.append(transition[4])
      prob_list_s.append(transition[5])
      val_list_s.append(transition[6])
    return np.array(state_list),action_list,reward_list,np.array(next_state_list),done_list,prob_list_s,val_list_s
  
  def size(self):
    return len(self.buffer)

class Actor_Network(keras.Model):
  def __init__(self,n_actions = total_actions, fc1_dims = 64, fc2_dims=64):
    super(Actor_Network,self).__init__()
    self.fc1_dims = fc1_dims
    self.fc2_dims = fc2_dims
    self.n_actions = total_actions
    self.fc1 = Dense(self.fc1_dims,activation='relu')
    self.fc2 = Dense(self.fc2_dims,activation='relu')
    self.pi = Dense(self.n_actions,activation='softmax')
    
  def call(self,state):
    pi_val = self.fc1(state)
    pi_val = self.fc2(pi_val)
    pi_val = self.pi(pi_val)
    return pi_val

class Critic_Network(keras.Model):
  def __init__(self,fc1_dims = 64,fc2_dims=64):
    super(Critic_Network,self).__init__()
    self.fc1_dims = fc1_dims
    self.fc2_dims = fc2_dims
    self.fc1 = Dense(fc1_dims,activation="relu")
    self.fc2 = Dense(fc2_dims,activation="relu")
    self.q = Dense(1,activation=None)
    
  def call(self,state):
    q_val = self.fc1(state)
    q_val = self.fc2(q_val)
    q_val = self.q(q_val)
    return q_val

class PPO_Agent():
  def __init__(self):
    self.actor = Actor_Network()
    self.actor.compile(optimizer=Adam(learning_rate = LEARNING_RATE_ACTOR))
    self.critic = Critic_Network()
    self.critic.compile(optimizer=Adam(learning_rate = LEARNING_RATE_CRITIC))
  
  def chose_action(self,state):
    state_tensor = tf.convert_to_tensor(state[None,:])
    action_probs = self.actor.call(state_tensor)
    action_prob_dist = tfp.distributions.Categorical(action_probs)
    action = action_prob_dist.sample()
    log_prob = action_prob_dist.log_prob(action)
    action = action.numpy()
    value=self.critic(state_tensor)
    value = value.numpy()
    log_prob = log_prob.numpy()
    return action[0],value[0],log_prob[0]

  def train(self,mini_batch):
    for _ in range(EPOCHS):
      state_batch,action_batch,reward_batch,next_state_batch,done_batch,old_log_probs,val_batch = mini_batch
      advantage_list = [0]*len(reward_batch)
      for i in range(len(reward_batch)-1):
        advantage = 0;
        discount = 1;
        for j in range(i,len(reward_batch)-1):
          advantage += discount*(reward_batch[j] + (1-int(done_batch[j]))*val_batch[j+1] - val_batch[j])
          discount*=GAMMA*LAMBDA
        advantage_list[i] = advantage[0]
      
      with tf.GradientTape() as tape:
        new_action_probs = self.actor.call(state_batch)
        new_action_dist = tfp.distributions.Categorical(new_action_probs)
        new_log_probs = new_action_dist.log_prob(action_batch)
        prob_ratio = tf.math.exp(new_log_probs - old_log_probs)
        objective_1 = advantage_list*prob_ratio
        objective_2 = tf.clip_by_value(prob_ratio,1-POLICY_CLIP,1+POLICY_CLIP)
        objective_2 = objective_2*advantage_list
        actor_loss = -tf.math.minimum(objective_1,objective_2)
        actor_loss = tf.math.reduce_mean(actor_loss)
      actor_network_gradient = tape.gradient(actor_loss,self.actor.trainable_variables)
      self.actor.optimizer.apply_gradients(zip(actor_network_gradient,self.actor.trainable_variables))

      with tf.GradientTape() as tape:
        val = tf.squeeze(self.critic.call(state_batch),1)
        critic_loss = tf.math.reduce_mean(val-advantage_list)**2

      critic_network_gradient = tape.gradient(critic_loss,self.critic.trainable_variables)
      self.critic.optimizer.apply_gradients(zip(critic_network_gradient,self.critic.trainable_variables))

agent = PPO_Agent()
memory = Replay_Memory()
score_list = []
score = 0
for n_ep in range(TOTAL_EPISODES):
    curr_state = env.reset()
    done=False
    while not done:
        action,val,log_prob = agent.chose_action(curr_state)
        next_state,reward,done,_=env.step(action)
        score+=reward
        memory.add_transition((curr_state,action,reward,next_state,done,log_prob,val))
        curr_state = next_state
    score_list.append(score)
    sample_batch = memory.sample()
    agent.train(sample_batch)
    print('num_eps: {} score: {}'.format(n_ep,score))
    score=0
env.close()

plt.plot(score_list)
plt.ylabel('Avg_Score')
plt.xlabel('Episode')
plt.show()

